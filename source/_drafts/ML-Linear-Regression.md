title    : 从线性回归开始
category : 机器学习
tags     : 
date     : 2014-11-14
---

### 什么是机器学习？

从Coursera课程中学习到，主要有下面两种定义：
- Arthur Samuel(1959):在进行特定编程的情况下，给予计算机学习能力的领域。
- Tom Mitchell(1998):一个程序被认为能够从经验E中学习，解决任务T，达到性能度量指标P。当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升。(例如在垃圾邮件识别系统中，对垃圾邮件的分类是T，对你的邮件标记为“垃圾邮件”或者标记为“不是垃圾邮件”是E，垃圾邮件分对的个数或者比例是P。)

机器学习算法按照类型也可以有以下分类:
- **有监督(supervised)学习**：我们将教计算机如何去完成任务
	* 回归问题：预测值连续（例如房价预测、销量预测）
	* 分类问题：预测值离散（例如垃圾邮件识别、账户是否异常）

	![](/assets/post/ML/ML_predict.png)

	如上图所示：我们可以看到，具体区分一个问题到底是分类问题还是回归问题的关键在于预测值是连续的还是离散的：对于房价预测问题的预测值自然便是房价了，房价可取值是连续的，代表着房价的数值，即预测值连续，因此是回归问题；对于垃圾邮件识别，预测值只有(1,-1)，代表是垃圾邮件和不是垃圾邮件，即预测值离散，因此是分类问题。
- **无监督(unsupervised)学习**：计算机自己去学习
	* 如聚类问题、语音提取
- 另外，也有**强化(Reinforcement)学习**、**推荐系统(Recommender System)**等，这些也都是机器学习算法。

### 机器学习的基本概念

机器学习用一句话来说就是利用训练集的数据，通过某种学习算法，最终得到一个模型表示。可以用下图来说明：
![](/assets/post/ML/Model Representation.png)
我们可以看出机器学习的训练过程包括了训练集、学习算法、模型等几个关键的模块，下来详细介绍下每个模块的功能。

**训练集——机器学习的来源**

训练集，我们可以认为是很多对(已知数据，预测结果)组成的集合，通过这个训练集我们可以获得一些“经验”，来让机器去学习。

例如，房价预测问题，每一条训练样本可以是`面积大小(X1)，房子位置(X2)，房子楼层(X3)，房价(Y)`，那么训练集可能是这样的：

		58, 西郊, 2, 6000
		98, 北郊, 3, 10000
		88, 西郊, 5, 9000
		78, 西郊, 3, 8000
		98, 南郊, 2, 10000
		58, 东郊, 1, 6100
		68, 西郊, 3, 6900

**学习算法——机器学习的方法**
有了学习的来源，自然而然的我们得教会机器去怎么去学习这些数据，这个学习数据的方法就是学习算法。

对于之前的房价预测问题，假设我们发现放假只和面积相关，那么我们或许可以用一条直线去拟合他，那么我们可能有如下假设：

$$
h_\theta(x) = \theta_0 + \theta_1 x_1
$$

那么$\theta_0$、$\theta_1$就是我们需要训练的参数，那么现在问题来了：**如何去得到这两个参数呢**？那么得到参数的方法就叫做学习算法。显而易见，最笨的方法就是我们试呗，用肉眼观察呗，觉得这条线差不多能代表这些数据了就OK了，那么好，这个方法就是机器学习的方法。

**模型——机器学习的结果**
一切都搞定了，得到的结果是什么呢？当然是模型了！

对于房价预测问题，我们或许可以训练出$\theta_0$、$\theta_1$，假设我们最终得到了这么一个公式：

$$
h_\theta(x) = 200 + 100 x_1
$$

这个公式有什么意义呢？没错，就是说房价等于100倍的面积再加200元。这个公式就是所谓的“预测模型”。假设这个模型十分准确的话，那么我们只要知道了房子的面积，就可以推测出房子大概的价格。在机器学习中，习惯用`Hypothesis`来表示模型的结果，因此，$h_\theta(x)$表示参数为$\theta$的模型，当然这个$\theta$可能为一个向量。

### 从线性回归开始

#### 模型描述
考虑下房价预测的问题，假设只有面积影响房子的价格，那么我们可以得到以下一个模型：

{% rawblock %}
\begin{aligned}
& Hypothesis: h_\theta(x) = \theta_0 + \theta_1 x_1 \\\
& Parameters: \theta_0, \theta_1 \\\
\end{aligned}
{% endrawblock %}

那么，思考下应该如何考核我们拟合的结果是否符合数据的需求呢？那么就有一个代价函数(Cost Function)的概念了，例如在本例中代价函数为：
{% rawblock %}
$$
J(\theta_0 ,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2
$$
{% endrawblock %}

顾名思义，这个函数表征的是一种代价。如果代价越低，说明模型拟合的程度越好，因此，我们需要达成的目标就是要最小化代价函数：
{% rawblock %}
$$
\min_{\theta_0, \theta_1} J(\theta_0, \theta_1)
$$
{% endrawblock %}

为了方便进一步学习，我们可以将模型化简一下，将$\theta_0$去掉，也就是得到了如下所示的模型与目标：

{% rawblock %}
\begin{aligned}
& Hypothesis: h_\theta(x) = \theta_1 x_1 \\\
& Parameters: \theta_1 \\\
& Cost funcrion: J(\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2\\\
& Goal: \min_{\theta_1} J(\theta_1)
\end{aligned}
{% endrawblock %}

#### 代价函数

代价函数如果越小，说明拟合的越完美，因此，当代价函数为0时，便是完全拟合，也就是说每一个样本点都落到了拟合的曲线上。我们可以尝试着计算一下代价函数，假设有3个样本点有`(1,1),(2,2),(3,3)`，对于$\theta_1=1, 0.5, 0$可以画出一下三个图：
![](/assets/post/ML/Cost Function.png)
对于$\theta_1=1$，我们可以得出$J(\theta_1)=\frac{1}{2*3}(0^2+0^2+0^2)=0$

对于$\theta_1=0.5$，我们可以得出$J(\theta_1)=\frac{1}{2*3}(\frac{1}{2}^2+1^2+\frac{3}{2}^2)=\frac{7}{12}$

对于$\theta_1=0$，我们可以得出$J(\theta_1)=\frac{1}{2*3}(1^2+2^2+3^2)=\frac{7}{3}$

那么，我们可以大概的画出代价函数$J(\theta)$的图像：
![](/assets/post/ML/Cost Function2.png)
当$\theta_1=1$时，取到代价函数的最小点。求这个最小点的过程就是机器学习训练的过程，那么，现在问题又来了，你真的要用那个一个一个点去试吗？！当然不是，那么就引出了下面的内容：梯度下降法。

#### 梯度下降法

梯度下降法利用负梯度方向作为搜索方向，迭代可以保证$\theta$向低值的地方搜索。

![](/assets/post/ML/Gradient.png)
因此，可以使用以下代码，对梯度进行计算：

Repeat until convergence {
{% rawblock %}
	$\theta_j := \theta_j - \alpha * \frac{\partial }{\partial \theta_j}J(\theta_0, ..., \theta_j)$
{% endrawblock %}
(注： 每次**同时**更新j个$\theta$) 
}

其中的，$\alpha$我们称之为**学习率**，学习率的设置太小，可能造成收敛速度太慢，学习率设置太大，可能错过最低点。

而这种梯度下降法，称为**Batch**方法，即每一步下降都使用全部的训练样本，在本例中，求$J(\theta)$时，需要使用全部的训练样本，因此，也是一种“Batch”方法。

#### 预测

当我们利用梯度下降法学习之后，得到了最终的模型时，就可以进行预测了，当输入一个$x$时，我们就可以根据模型相应的输出一个$y$了。

以上便是一个简单的机器学习例子，它利用梯度下降法求得了线性模型的参数。