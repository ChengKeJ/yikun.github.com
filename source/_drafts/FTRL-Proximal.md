title    : Ad Click Prediction： a View from the Trenches[译]
category : 算法学习
tags     : 
date     : 2014-11-20
---

### 概述

广告点击率(Click–Through Rates,CTR)预测是一个在数十亿美元在线广告产业中极其重要的学习算法。我们从目前已经部署的CTR预测系统的最近试验中，精选了一些案例和话题做了介绍。这里面包括了对传统的监督学习算法进行的改进，一种基于FTRL-Proximal的的在线学习算法(具有出色的稀疏性和收敛性)，还有在每一维度使用不同学习率。

我们也探索了一些出现在现实系统中的挑战，这可能超出了传统的机器学习研究的主要领域。包括了一些内存节省的技巧，评估方法及可视化的性能，点击率可信评估的实践方法，自动化管理特征的方法。最后，我们也详细的介绍了一些在其他著作中提到的结论，但是对我们没有什么益处的方向。这篇论文的亮点在于将理论与实践结合，并且展示了在复杂的动态系统中使用传统机器学习算法的深度挑战。

分类：[计算方法学]: 模式识别应用

关键词：在线广告 数据挖掘 大规模学习

### 1. 介绍

在线广告是一个数十亿美元的产业，机器学习在这个领域中有很多成功的故事。搜索广告、上下文广告、展示广告以及实时竞价排名等都非常依赖学习模型在广告点击率预测中的准确、快速、可靠。这个问题出现也推动了该领域要解决问题的规模，这在十年前几乎是不可想象的。一个典型的工业模型可能每天会提供数十亿次的预测，同时也使用了一个同规模的特征空间，再从大规模的数据中学习。

这篇论文，我们展示了一系列案例，这些案例来自于部署在Google搜索广告的点击率预测的系统中。因为这个问题已经很好的解决了，因此我们选择一系列关注度很少，但是在实际系统的特别有用的话题。因此，我们探索了内存节省、性能分析、预测可信度、矫正以及特征管理等问题，按照传统问题的严格要求，有效地设计一种高效的学习算法。这篇论文的目的是给读者介绍一些在工业界遇到的深度挑战，同时也分享了我们在大规模问题领域的一些技巧和见解。

### 2. 系统概览

当用户搜索$q$时，一些初始的广告会根据广告主选择的一些关键词与搜索$q$匹配。然后，一个竞价系统会决定这个广告是否展示给用户、以怎样的顺序展示给用户，如果用户点击了这个广告，广告主需要支付多少钱。除了广告商出价外，一个很重要的输入便是每个广告$a$对应的点击率$P(click | q, a)$，即这个广告被展示的可能性。

系统中使用的特征往往来自于多种不同来源，包括了查询、广告文字、素材以及各种不同的广告相关的元数据。这些数据往往十分稀疏，每个样本可能只有很小一部分的非零值。

正则化的逻辑回归方法十分适用于这个问题。每天需要进行好几十亿次的预测，也要根据新的点击、非点击来快速的更新模型。当然这样的数据量也就意味着训练数据是巨大的。一个基于流服务的Photon系统提供着这些数据。

由于大规模的学习已经在这几年有了很好的研究，因此这里不打算在这篇文章内花大量篇幅去详细介绍我们的系统。需要注意的是，虽然我们的方法和谷歌大脑团队提出的Downpour SGD(并行的随机梯度下降)方法有相似之处，而不同之处在与我们训练的是一个单层的模型，而不是具有很多层的深度网络。这允许我们能够处理更大的数据和模型，而不是到处去报告这些具有数十亿系数的“知识”。因为我们的训练模型需要复制到很多数据中心去服务(如图1)，我们更关心的是服务时候的稀疏化，而不是训练时的稀疏。

### 3. 在线学习和稀疏

对于在大数据的学习，在线算法对于线性模型(例如逻辑回归)的生成有很多的好处。尽管特征向量可能有数十亿维度，典型的样本可能只有几百个非零值。这一特性使得对通过磁盘或者网络，以数据流方式传输的大数据集的高效训练有了可能，每一个训练样本只需要被考虑一次。

{% rawblock %}
为了正确明了的表示这个算法，我们需要进行一些符号的声明。我们设向量$g_t \in  \mathbb{R}^d$。$t$表示当前训练样本的索引，向量$g_t$的第$i^{th}$项表示为$g_{t,i}$，我们也用$g_{1:t}=\sum\nolimits_{s=1}^tg_s$。
{% endrawblock %}


如果我们希望利用逻辑回归来建立问题的模型，我们可以使用下面的在线框架。在每一个$t$中，我们需要预测被描述为$X_t \in \mathbb{R}^d$特征向量的样本；给一系列模型参数$W_t$，我们预测$p_t=\sigma(W_t\cdot X_t)$，其中$\sigma(a)=1/(1+exp(-a))$是sigma函数。然后，我们需要观察标记$y_t \in \lbrace 0,1 \rbrace$，然后计算LogLoss（对数损失）:
$$
l_t(W_t)=-y_t \log p_t-(1-y_t)\log(1-p_t), (1)
$$

很明显可以看出$\nabla l_t(W)=(\sigma(W\cdot X_t)-y_t)Xt=(p_t-y_t)X_t$，而这个梯度便是我们需要优化的目标。

在线梯度下降法(OGD)在一些问题中已经被证明很高效，它可以利用很少的计算资源得到一个不错的准确度。然而，在实践中，另外一个关键点在于最终的模型；模型需要被稀疏的存储，那么在$w$中的非零系数是决定内存消耗的关键。

不幸的是，OGD并不是特别高效的产生稀疏模型。实际上，简单的在损失函数($\nabla_Wl_t(W)$)中增加$L_1$范数，从本质上来说不可能产生出绝对零值的系数。大多数比较有经验的做法比如FOBOS和截断梯度，可以成功的产生稀疏解。RDA(Regularized Dual Averaging)算法则对于准确性与稀疏性的权衡比FOBOS更好。然而，我们发现在我们的数据集上梯度下降法要比RDA的准确率更高。那么问题来了，我们是否既能像RDA一样保证稀疏性，又可以像OGD一样保证准确性呢？答案是肯定的，那就是使用FTRL(Follow The Proximaly Regularized Leader)算法，或者叫FTRL-Proximal。如果没有正则化的话，那么这个算法与在线梯度下降法相同，但是因为使用了另一种lazy的模型来表示系数$w$，$L1$正则化可以被实现的更高效。

FTRL-Proximal算法之前，表示为一种利于理论研究的方式，现在，我们更专注于描述一种工程实现方法。给一系列的梯度$g_t \in \mathbb{R}$，OGD会更新

$$
W_{t+1}=W_t-\eta_tg_t
$$

其中，$\eta_t$是一个非增学习速率，比如$\eta_t=\frac{1}{\sqrt t}$。FTRL-Proximal算法则更新
{% rawblock %}
$$
W_{t+1}=argmin_W(g_{1:t}\cdot W + \frac{1}{2}\sum_{s=1}^t \sigma_s\|W-W_s\|_2^2+\lambda_1\|W\|_1)
$$
{% endrawblock %}

